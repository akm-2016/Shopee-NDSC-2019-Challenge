{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDSC-2019\n",
    "## Image Classification Using Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prerequisite</b> \n",
    "You have run the Mobile_traindata.ipynb notebook and has training and test data ready in the same directory. This includes images and csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "currentdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(currentdir, r'train_data.csv'), index_col=0)\n",
    "train1 = train[:10000]\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare image dimensions. For VGG16 and Resnet50 use 224x224, for InceptionResNetV2 use 299x299\n",
    "#we are using coloured images, so channel=3 for R,B,G. \n",
    "nrows = 224\n",
    "ncolumns = 224\n",
    "channels = 3  #change to 1 for grayscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert each image to numpy array after resizing to 100,100 size (for train data)\n",
    "def func(path):\n",
    "    img = image.load_img(path, target_size=(img_rows, img_cols))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array /= 255 #standardizing array values between 0.0 to 1.0\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1['img_array'] = train1['image_path'].apply(lambda x: func(x))\n",
    "\n",
    "print(train1.shape)\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our image is now represented by a NumPy array of shape (224, 224, 3),assuming TensorFlow \"channels last\" ordering of course\n",
    "# but we need to expand the dimensions to be (1, 3, 224, 224) so we can pass it through the network\n",
    "# we'll also preprocess the image by subtracting the mean RGB pixel intensity from the ImageNet dataset\n",
    "\n",
    "#image = np.expand_dims(image, axis=0)\n",
    "#image = preprocess_input(image)  #uncomment and run again to see the difference, if it improves accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If at this stage we inspect the shape of our image, you’ll notice the shape of the NumPy array is (3, 224, 224) — each image is 224 pixels wide, 224 pixels tall, and has 3 channels (one for each of the Red, Green, and Blue channels, respectively).\n",
    "\n",
    "However, before we can pass our image through our CNN for classification, we need to expand the dimensions to be (1, 3, 224, 224). This is done because when classifying images using Deep Learning and Convolutional Neural Networks, we often send images through the network in “batches” for efficiency. Thus, it’s actually quite rare to pass only one image at a time through the network.\n",
    "\n",
    "We then preprocess the image by subtracting the mean RGB pixel intensity computed from the ImageNet dataset. Done in last line of above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load train data into arrays in the shape(num_rows, img_height, img_width, num_channels)\n",
    "x=[]\n",
    "y=[]\n",
    "for i,rows in train1.iterrows():\n",
    "    x.append(rows[2])\n",
    "    y.append(rows[1])\n",
    "\n",
    "x_train= np.array(x)\n",
    "print(x_train.shape)\n",
    "y_train = np.array(y)\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#validation data\n",
    "(x_train, x_valid) = x_train[500:], x_train[:500] \n",
    "(y_train, y_valid) = y_train[500:], y_train[:500]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First check with VGG16, if accuracy is below 95% even after tuning parameters then use this model\n",
    "# This works well on large dataset but overfits on small ones\n",
    "# Uncomment the below two lines to run it\n",
    "\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "#conv_base = ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the data is extremely huge and want to improve accuracy more try this model\n",
    "# This model is very heavy on computation power and will take hours and hours to train the model\n",
    "# Uncomment the below two lines to run it\n",
    "\n",
    "#from keras.applications import InceptionResNetV2\n",
    "\n",
    "#conv_base = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "predictions = Dense(35, activation='softmax')(x) #35 is the number of categories/classes that we have to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can try RMSprop optimizer with a learning rate of 0.0001 also\n",
    "#We'll use binary_crossentropy loss because its a binary classification\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-5), metrics=['acc'])\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['acc'])\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.adam(lr=0.0001), metrics=['acc'])\n",
    "\n",
    "# We can check the model with SGD(lr=0.0001, momentum=0.9) and adam(lr=0.0001) to check accuracy. Uncomment above lines for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train , batch_size = 64, epochs = 10 , \n",
    "          validation_data=(x_valid, y_valid), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the model (optional)\n",
    "model.save_weights('model_wieghts.h5')\n",
    "model.save('model_keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the details form the history object\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#Train and validation accuracy\n",
    "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
    "plt.title('Training and Validation accurarcy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "#Train and validation loss\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's create a function that makes our plot looks smoother and cleaner.\n",
    "def smooth_plot(points, factor=0.7):\n",
    "    smooth_pts = []\n",
    "    for point in points:\n",
    "        if smooth_pts:\n",
    "            previous = smooth_pts[-1]\n",
    "            smooth_pts.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smooth_pts.append(point)\n",
    "    return smooth_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot figure\n",
    "plt.plot(epochs, smooth_plot(acc), 'b', label='Training accurarcy')\n",
    "plt.plot(epochs, smooth_plot(val_acc), 'r', label='Validation accurarcy')\n",
    "plt.title('Training and Validation accurarcy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read test data\n",
    "test = pd.read_csv(os.path.join(currentdir, r'test_data.csv'), index_col = 0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert images to np array\n",
    "\n",
    "test['img_array'] = test['image_path'].apply(lambda x: func(x))\n",
    "\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preparing test data\n",
    "x1=[]\n",
    "\n",
    "for i,rows in test.iterrows():\n",
    "    x1.append(rows[4])\n",
    "\n",
    "x_test= np.array(x1)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "P = decode_predictions(preds, top=3)  # top is optional, if want want to see top 3 prediction probabilities, else omit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Accuracy\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
